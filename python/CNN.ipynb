{
 "metadata": {
  "name": "",
  "signature": "sha256:5fa6be4e6805be748e13a763f6a283271867f47e41e80d3e706f317d4157cbfd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Deepcuts CNN"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a fully self-contained CNN Architecture for shot boundary detection. The code performs the following tasks:\n",
      "\n",
      "- Create a FrameDataSet class inheriting 'DenseDesignMatrix' to hold the data. The Pylearn2 CNN architecture expects a DenseDesignMatrix container.\n",
      "\n",
      "- Read all data files\n",
      "\n",
      "- Create an ndarray, frame_pair, of dimension Channel x Height x Width for the data of each frame-pair in both the train & test set. The format of the array is as follows:\n",
      "    - Channel = The signal type (e.g. horizontal flow)\n",
      "    - Height = The height of the image \n",
      "    - Width = The Width of the images\n",
      "\n",
      "- Put all of the frame_pairs into a FrameDataSet\n",
      "\n",
      "- Define the network architecture using the YAML markup language\n",
      "\n",
      "- Run the CNN"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dependencies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- <a href=http://deeplearning.net/software/pylearn2/> Pylearn2</a>\n",
      "\n",
      "- <a href=http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/#axzz3ZD4QUCPt> CUDA (optional)</a>\n",
      "    - <a href=http://www.quantstart.com/articles/Installing-Nvidia-CUDA-on-Ubuntu-14-04-for-Linux-GPU-Computing> A helpful CUDA install guide </a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preparation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- <a href=http://deeplearning.net/software/theano/tutorial/using_gpu.html> Theano GPU Settings </a> (if using cuda)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "References & Helpful Links"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- <a href=http://deeplearning.net/software/pylearn2/tutorial/index.html#tutorial> Python Quick Start Tutorial </a>\n",
      "- <a href=http://deeplearning.net/software/pylearn2/yaml_tutorial/index.html#yaml-tutorial> YAML for Pylearn2</a>\n",
      "- <a href=http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/convolutional_network/convolutional_network.ipynb>iPython Notebook CNN Tutorial</a>\n",
      "- <a href=http://nbviewer.ipython.org/gist/cancan101/ea563e394ea968127e0e > Fully Contained iPython Notebook CNN </a>\n",
      "- <a href=https://github.com/kastnerkyle/kaggle-cifar10/blob/master/kaggle_dataset.py> Sample Dataset Class </a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Put all of the imports here\n",
      "#NOTE: Importing all of pylearn2 is VERY slow. \n",
      "# - Import only the needed components.\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "import numpy as np\n",
      "from scipy import misc\n",
      "\n",
      "import theano\n",
      "\n",
      "from theano.compat.six.moves import xrange\n",
      "from pylearn2.datasets import dense_design_matrix\n",
      "from pylearn2.datasets import control\n",
      "from pylearn2.datasets import cache\n",
      "from pylearn2.utils import serial\n",
      "from pylearn2.utils.rng import make_np_rng\n",
      "\n",
      "from os import listdir\n",
      "from os.path import isfile, isdir, join, basename, splitext\n",
      "\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import sys\n",
      "import Image\n",
      "\n",
      "import csv\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Check which device theano uses (gpu or cpu\n",
      "print theano.config.device"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cpu\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Define Global Parameters\n",
      "TRAIN_DATA = '/home/alex/Desktop/deepcuts_data/signals/'\n",
      "TEST_DATA = '/home/alex/Desktop/deepcuts_data/signals/'\n",
      "TRAIN_LABELS = \"/home/alex/Desktop/deepcuts_data/shot_annot/csv/\"\n",
      "TEST_LABELS = \"/home/alex/Desktop/deepcuts_data/shot_annot/csv/\"\n",
      "\n",
      "\n",
      "FORCED_HEIGHT = 32\n",
      "FORCED_WIDTH = 32\n",
      "\n",
      "BATCH_COUNT = 100\n",
      "\n",
      "SIGNALS = ['sample_signal']\n",
      "SIG_COUNT = 3\n",
      "\n",
      "#Signals that should be converted to single channel\n",
      "NEEDS_COLOR_CONVERSION = ['h_flow', 'v_flow', 'luminance'] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Each pixel value is the average from rgb\n",
      "#This is for mathematical accuracy\n",
      "#This is NOT for producing visually accurate rgb2gray conversion\n",
      "def convert(signal_type, im):\n",
      "    \n",
      "    im = im.resize((FORCED_HEIGHT, FORCED_WIDTH))\n",
      "    image = np.asarray(im)\n",
      "    \n",
      "    h,w,ch = image.shape\n",
      "    assert (ch < h and ch < w), 'Invalid Input Size: Expected data input is of size H x W x Ch'\n",
      "    \n",
      "    if signal_type in NEEDS_COLOR_CONVERSION:\n",
      "        output = np.zeros([h,w])\n",
      "        for i in range(0, ch):\n",
      "            output = output + image[:,:,i]\n",
      "        \n",
      "        image = output / ch        \n",
      "    #After color conversion, we reshape for CNN input. \n",
      "    #CNN Expects ndarry to be N x Channel x Height x Width\n",
      "    \n",
      "    #move channels to front axis\n",
      "    image = image.swapaxes(0,2) #Now image is Ch x W x H\n",
      "    \n",
      "    #swap W and H\n",
      "    image = image.swapaxes(1,2) #Now image is Ch x H x W\n",
      "    \n",
      "    return image\n",
      "    \n",
      "            \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Get all files in directory\n",
      "def get_all_files(path):\n",
      "    files = [f for f in listdir(path) if isfile(join(path, f)) and \\\n",
      "             f[0] != '.' ] #ignore hidden files\n",
      "    return sorted(files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Get all subdirectories in a directory\n",
      "def get_all_dirs(path):\n",
      "    folders = [f for f in listdir(path) if isdir(join(path, f)) and \\\n",
      "               f[0] != '.'] #ignore hiddin directories\n",
      "    return sorted(folders)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get all signal files for movie in directory\n",
      "def get_images(movie, signal, data_path):\n",
      "    path = data_path + movie + '/' + signal + '/'\n",
      "    files = get_all_files(path)\n",
      "    image_dict = {}\n",
      "    \n",
      "    for f in files:\n",
      "        im = Image.open(path + f)\n",
      "        im.load()\n",
      "        image_dict[int(f[:-4])] = convert(signal, im)\n",
      "        \n",
      "    return image_dict\n",
      "\n",
      "#get_images('anni005', 'sample_signal')\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_movie_signals(movie, data_path):\n",
      "    mov_sigs = {}\n",
      "    for s in SIGNALS:\n",
      "        mov_sigs[s] = get_images(movie, s, data_path)\n",
      "        \n",
      "    #now vstack them. \n",
      "    mov_compiled = mov_sigs[SIGNALS[0]]\n",
      "    \n",
      "    for f in mov_compiled:\n",
      "        for s in SIGNALS[1:]:\n",
      "            mov_compiled[f] =  np.vstack(mov_compiled[f], mov_sigs[s])\n",
      "            \n",
      "    return mov_compiled\n",
      "    \n",
      "    \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_all_sigs(data_path):\n",
      "    dirs = get_all_dirs(data_path)\n",
      "    print('Found ' + str(len(dirs)) + ' movies: ')\n",
      "    print(dirs)\n",
      "    all_sigs = {}\n",
      "    for d in dirs:\n",
      "        all_sigs[d] = get_movie_signals(d, data_path)\n",
      "    \n",
      "    return all_sigs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Read shot_annot csv files\n",
      "\n",
      "# Dictionary of all movies\n",
      "# Each movie will have a list of ShotAnnotations\n",
      "# read the files into dictionary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def get_labels(label_path):\n",
      "    \n",
      "    files = [f for f in listdir(label_path) if isfile(join(label_path, f)) and f[0] != '.']\n",
      "    annotations = {}\n",
      "    annot_arrays = {}\n",
      "    colNames = ['style', 'pre_frame', 'post_frame']\n",
      "    \n",
      "    for f in files:\n",
      "        full_path = label_path + f\n",
      "        name = (basename(f)).split('.')[0] #get only movie title\n",
      "        annotations[name] = []\n",
      "        annot_arrays[name] = {}\n",
      "        with open(label_path + f, 'rt') as csvfile:\n",
      "            reader = csv.reader(csvfile, delimiter=',')\n",
      "            next(reader)    \n",
      "            idx = 0\n",
      "            for row in reader:\n",
      "                #new_annot = ShotAnnotation(f, row[0], row[1], row[2])\n",
      "                #annotations[name].append(new_annot)\n",
      "                if(row[0] == 'CUT'):\n",
      "                    annot_arrays[name][row[1]] = 1\n",
      "\n",
      "    #Debug check: did the dictionary populate as expected?        \n",
      "    print('Gathered labels for ',len(annot_arrays), 'videos')\n",
      "    return annot_arrays\n",
      "    \n",
      "    \n",
      "#get_labels()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Make the final image and label arrays\n",
      "\n",
      "def prep_data(data_path, label_path):\n",
      "    signals = get_all_sigs(data_path)\n",
      "    labels = get_labels(label_path)\n",
      "    \n",
      "    final_labels = []\n",
      "    signal_ndArrays = np.empty((1, SIG_COUNT, FORCED_HEIGHT, FORCED_WIDTH))\n",
      "    for m in sorted(signals):\n",
      "        max_val = max([int(k) for k in signals[m]])\n",
      "        \n",
      "        these_labels = [0] * (max_val - 1)\n",
      "        for cut in labels[m]:\n",
      "            #-1 because frames are indexed starting at 1\n",
      "            #these_labels[int(cut) - 1] = labels[cut]\n",
      "            x = 1\n",
      "        final_labels.append(these_labels)\n",
      "        itermov = iter(sorted(signals[m]))\n",
      "        \n",
      "        if(len(signal_ndArrays) == 1):\n",
      "            signal_ndArrays[0,:,:,:] = signals[m][1]\n",
      "            next(itermov)\n",
      "            \n",
      "        for sig in itermov:\n",
      "            new_array = np.empty((1, SIG_COUNT, FORCED_HEIGHT, FORCED_WIDTH))\n",
      "            new_array[0,:,:,:] = signals[m][sig]\n",
      "            signal_ndArrays = np.vstack((signal_ndArrays, new_array))\n",
      "            \n",
      "    print final_labels\n",
      "    print(signal_ndArrays.shape)\n",
      "    \n",
      "    return{'labels':final_labels, 'signals':signal_ndArrays}\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create the FrameDataSet Class\n",
      "class FrameDataSet(dense_design_matrix.DenseDesignMatrix):\n",
      "    \n",
      "    def __init__(self, data_path, label_path):\n",
      "        data = prep_data(data_path, label_path)\n",
      "        y = np.asarray(data['labels'])\n",
      "        print(y.size)\n",
      "        X = data['signals']\n",
      "        \n",
      "        self.num_ims, self.ch, self.h, self.w = X.shape\n",
      "        self.img_shape = (self.ch, self.h, self.w)\n",
      "        start_idx = 0\n",
      "        self.max_count = sys.maxsize\n",
      "        self.label_names = [0, 1]\n",
      "        self.n_classes = len(self.label_names)    \n",
      "            \n",
      "            \n",
      "        #The following parts might not be necessary.\n",
      "        self.one_hot = False\n",
      "        self.gcn = 55. #Don't know what this does.\n",
      "        \n",
      "    \n",
      "        def get_labels():\n",
      "            return labels\n",
      "        \n",
      "        self.label_map = {k: v for k, v in zip(self.label_names,\\\n",
      "                                               range(self.n_classes))}\n",
      "        self.label_unmap = {v: k for k, v in zip(self.label_names,\\\n",
      "                                               range(self.n_classes))}\n",
      "        \n",
      "        super(FrameDataSet, self).__init__(y=y,\n",
      "                                           topo_view=X)\n",
      "        \n",
      "        \n",
      "        \n",
      "                                         \n",
      "            \n",
      "    \n",
      "            \n",
      "        \n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#all_data = FrameDataSet(TRAIN_DATA, TRAIN_LABELS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "from pylearn2.models import mlp\n",
      "from pylearn2.costs.mlp.dropout import Dropout\n",
      "from pylearn2.training_algorithms import sgd, learning_rule\n",
      "from pylearn2.termination_criteria import EpochCounter\n",
      "from pylearn2.datasets.preprocessing import Pipeline, ZCA\n",
      "from pylearn2.datasets.preprocessing import GlobalContrastNormalization\n",
      "from pylearn2.space import Conv2DSpace\n",
      "from pylearn2.train import Train\n",
      "from pylearn2.train_extensions import best_params, window_flip\n",
      "from pylearn2.utils import serial\n",
      "\n",
      "#Define Datasets here\n",
      "trn = FrameDataSet(TRAIN_DATA, TRAIN_LABELS)\n",
      "\n",
      "tst = FrameDataSet(TEST_DATA, TEST_LABELS)\n",
      "\n",
      "\n",
      "#Define the network here\n",
      "in_space = Conv2DSpace(shape=(32, 32),\n",
      "                       num_channels=3,\n",
      "                       axes=('c', 0, 1, 'b'))\n",
      "\n",
      "l1 = mlp.ConvRectifiedLinear(layer_name = 'h2',\n",
      "                     output_channels=64 ,\n",
      "                     irange= .05,\n",
      "                     kernel_shape= (5, 5),\n",
      "                     pool_shape= (4, 4),\n",
      "                     pool_stride= (2, 2),\n",
      "                     max_kernel_norm= 1.9365)\n",
      "\n",
      "l2 = mlp.ConvRectifiedLinear(layer_name = 'h3',\n",
      "                     output_channels=64 ,\n",
      "                     irange= .05,\n",
      "                     kernel_shape= (5, 5),\n",
      "                     pool_shape= (4, 4),\n",
      "                     pool_stride= (2, 2),\n",
      "                     max_kernel_norm= 1.9365)\n",
      "\n",
      "\n",
      "output = mlp.Softmax(layer_name='y',\n",
      "                     n_classes=2,\n",
      "                     irange=.005,\n",
      "                     max_col_norm=1.9365)\n",
      "\n",
      "layers = [l1, l2, output]\n",
      "\n",
      "#Define the model\n",
      "#Not sure if we need to modify this.\n",
      "mdl = mlp.MLP(layers,\n",
      "              input_space=in_space)\n",
      "\n",
      "\n",
      "#Not sure what this does from here to next comment.\n",
      "trainer = sgd.SGD(learning_rate=.17,\n",
      "                  batch_size=32,\n",
      "                  learning_rule=learning_rule.Momentum(.5),\n",
      "                  # Remember, default dropout is .5\n",
      "                  cost=Dropout(input_include_probs={'l1': .8},\n",
      "                               input_scales={'l1': 1.}),\n",
      "                  termination_criterion=EpochCounter(max_epochs=475),\n",
      "                  monitoring_dataset={'valid': tst,\n",
      "                                      'train': trn})\n",
      "\n",
      "preprocessor = Pipeline([GlobalContrastNormalization(scale=55.), ZCA()])\n",
      "trn.apply_preprocessor(preprocessor=preprocessor, can_fit=True)\n",
      "tst.apply_preprocessor(preprocessor=preprocessor, can_fit=False)\n",
      "serial.save('deepcuts_preprocessor.pkl', preprocessor)\n",
      "\n",
      "watcher = best_params.MonitorBasedSaveBest(\n",
      "    channel_name='valid_y_misclass',\n",
      "    save_path='deepcuts_maxout_zca.pkl')\n",
      "\n",
      "velocity = learning_rule.MomentumAdjustor(final_momentum=.65,\n",
      "                                          start=1,\n",
      "                                          saturate=250)\n",
      "\n",
      "decay = sgd.LinearDecayOverEpoch(start=1,\n",
      "                                 saturate=500,\n",
      "                                 decay_factor=.01)\n",
      "'''\n",
      "win = window_flip.WindowAndFlipC01B(pad_randomized=8,\n",
      "                                    window_shape=(32, 32),\n",
      "                                    randomize=[trn],\n",
      "                                    center=[tst])\n",
      "'''\n",
      "#Define experiment\n",
      "experiment = Train(dataset=trn,\n",
      "                   model=mdl,\n",
      "                   algorithm=trainer,\n",
      "                   extensions=[watcher, velocity, decay])\n",
      "\n",
      "#Run experiment\n",
      "experiment.main_loop()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 1 movies: \n",
        "['anni005']\n",
        "('Gathered labels for ', 13, 'videos')\n",
        "[[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
        "(4, 3, 32, 32)\n",
        "9\n",
        "Found 1 movies: \n",
        "['anni005']\n",
        "('Gathered labels for ', 13, 'videos')\n",
        "[[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
        "(4, 3, 32, 32)\n",
        "9\n",
        "Input shape: (32, 32)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Detector space: (28, 28)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Output space: (13, 13)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Input shape: (13, 13)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Detector space: (9, 9)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Output space: (4, 4)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "computing zca of a (4, 3072) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cov estimate took 0.0535180568695 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "eigh() took 20.4604871273 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/alex/pylearn2/pylearn2/train.py:85: UserWarning: dataset has no yaml src, model won't know what data it was trained on\n",
        "  \"data it was trained on\")\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "MLP has no layer(s) named l1",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-40-eafd73058ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m#Run experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/alex/pylearn2/pylearn2/train.pyc\u001b[0m in \u001b[0;36mmain_loop\u001b[0;34m(self, time_budget)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m    140\u001b[0m         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mextension_continue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_callbacks_and_monitoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/alex/pylearn2/pylearn2/train.pyc\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_budget_exceeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_extensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/alex/pylearn2/pylearn2/training_algorithms/sgd.pyc\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, model, dataset)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         cost_value = self.cost.expr(model, nested_args,\n\u001b[0;32m--> 316\u001b[0;31m                                     ** fixed_var_descr.fixed_vars)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcost_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcost_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/alex/pylearn2/pylearn2/costs/mlp/dropout.pyc\u001b[0m in \u001b[0;36mexpr\u001b[0;34m(self, model, data, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mdefault_input_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_input_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0minput_scales\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_scales\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mper_example\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         )\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/alex/pylearn2/pylearn2/models/mlp.pyc\u001b[0m in \u001b[0;36mdropout_fprop\u001b[0;34m(self, state_below, default_input_include_prob, input_include_probs, default_input_scale, input_scales, per_example)\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0minput_scales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_layer_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_include_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_layer_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_scales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/alex/pylearn2/pylearn2/models/mlp.pyc\u001b[0m in \u001b[0;36m_validate_layer_names\u001b[0;34m(self, layers)\u001b[0m\n\u001b[1;32m    960\u001b[0m                              if layer not in self.layer_names]\n\u001b[1;32m    961\u001b[0m             raise ValueError(\"MLP has no layer(s) named %s\" %\n\u001b[0;32m--> 962\u001b[0;31m                              \", \".join(unknown_names))\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_total_input_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: MLP has no layer(s) named l1"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}