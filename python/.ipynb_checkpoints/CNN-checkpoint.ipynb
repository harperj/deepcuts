{
 "metadata": {
  "name": "",
  "signature": "sha256:31e2005085d3d8c08ffa3f93bb4923d44cd6974f9d729cef1d66de7f0a4f5874"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Deepcuts CNN"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a fully self-contained CNN Architecture for shot boundary detection. The code performs the following tasks:\n",
      "\n",
      "- Create a FrameDataSet class inheriting 'DenseDesignMatrix' to hold the data. The Pylearn2 CNN architecture expects a DenseDesignMatrix container.\n",
      "\n",
      "- Read all data files\n",
      "\n",
      "- Create an ndarray, frame_pair, of dimension Channel x Height x Width for the data of each frame-pair in both the train & test set. The format of the array is as follows:\n",
      "    - Channel = The signal type (e.g. horizontal flow)\n",
      "    - Height = The height of the image \n",
      "    - Width = The Width of the images\n",
      "\n",
      "- Put all of the frame_pairs into a FrameDataSet\n",
      "\n",
      "- Define the network architecture using the YAML markup language\n",
      "\n",
      "- Run the CNN"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dependencies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- <a href=http://deeplearning.net/software/pylearn2/> Pylearn2</a>\n",
      "\n",
      "- <a href=http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/#axzz3ZD4QUCPt> CUDA (optional)</a>\n",
      "    - <a href=http://www.quantstart.com/articles/Installing-Nvidia-CUDA-on-Ubuntu-14-04-for-Linux-GPU-Computing> A helpful CUDA install guide </a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preparation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- <a href=http://deeplearning.net/software/theano/tutorial/using_gpu.html> Theano GPU Settings </a> (if using cuda)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "References & Helpful Links"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- <a href=http://deeplearning.net/software/pylearn2/tutorial/index.html#tutorial> Python Quick Start Tutorial </a>\n",
      "- <a href=http://deeplearning.net/software/pylearn2/yaml_tutorial/index.html#yaml-tutorial> YAML for Pylearn2</a>\n",
      "- <a href=http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/convolutional_network/convolutional_network.ipynb>iPython Notebook CNN Tutorial</a>\n",
      "- <a href=http://nbviewer.ipython.org/gist/cancan101/ea563e394ea968127e0e > Fully Contained iPython Notebook CNN </a>\n",
      "- <a href=https://github.com/kastnerkyle/kaggle-cifar10/blob/master/kaggle_dataset.py> Sample Dataset Class </a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Put all of the imports here\n",
      "#NOTE: Importing all of pylearn2 is VERY slow. \n",
      "# - Import only the needed components.\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "import numpy as np\n",
      "from scipy import misc\n",
      "\n",
      "import theano\n",
      "\n",
      "from theano.compat.six.moves import xrange\n",
      "from pylearn2.datasets import dense_design_matrix\n",
      "from pylearn2.datasets import control\n",
      "from pylearn2.datasets import cache\n",
      "from pylearn2.utils import serial\n",
      "from pylearn2.utils.rng import make_np_rng\n",
      "\n",
      "from os import listdir\n",
      "from os.path import isfile, isdir, join, basename, splitext\n",
      "\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import sys\n",
      "import Image\n",
      "\n",
      "import csv\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 227
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Check which device theano uses (gpu or cpu\n",
      "print theano.config.device"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cpu\n"
       ]
      }
     ],
     "prompt_number": 228
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Define Global Parameters\n",
      "TRAIN_DATA = '/home/alex/Desktop/deepcuts_data/signals/'\n",
      "TEST_DATA = '/home/alex/Desktop/deepcuts_data/signals/'\n",
      "TRAIN_LABELS = \"/home/alex/Desktop/deepcuts_data/shot_annot/csv/\"\n",
      "TEST_LABELS = \"/home/alex/Desktop/deepcuts_data/shot_annot/csv/\"\n",
      "\n",
      "\n",
      "FORCED_HEIGHT = 512\n",
      "FORCED_WIDTH = 512\n",
      "\n",
      "SIGNALS = ['sample_signal']\n",
      "SIG_COUNT = 3\n",
      "\n",
      "#Signals that should be converted to single channel\n",
      "NEEDS_COLOR_CONVERSION = ['h_flow', 'v_flow', 'luminance'] \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 246
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Each pixel value is the average from rgb\n",
      "#This is for mathematical accuracy\n",
      "#This is NOT for producing visually accurate rgb2gray conversion\n",
      "def convert(signal_type, im):\n",
      "    \n",
      "    im = im.resize((FORCED_HEIGHT, FORCED_WIDTH))\n",
      "    image = np.asarray(im)\n",
      "    \n",
      "    h,w,ch = image.shape\n",
      "    assert (ch < h and ch < w), 'Invalid Input Size: Expected data input is of size H x W x Ch'\n",
      "    \n",
      "    if signal_type in NEEDS_COLOR_CONVERSION:\n",
      "        output = np.zeros([h,w])\n",
      "        for i in range(0, ch):\n",
      "            output = output + image[:,:,i]\n",
      "        \n",
      "        image = output / ch        \n",
      "    #After color conversion, we reshape for CNN input. \n",
      "    #CNN Expects ndarry to be N x Channel x Height x Width\n",
      "    \n",
      "    #move channels to front axis\n",
      "    image = image.swapaxes(0,2) #Now image is Ch x W x H\n",
      "    \n",
      "    #swap W and H\n",
      "    image = image.swapaxes(1,2) #Now image is Ch x H x W\n",
      "    \n",
      "    return image\n",
      "    \n",
      "            \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 247
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Get all files in directory\n",
      "def get_all_files(path):\n",
      "    files = [f for f in listdir(path) if isfile(join(path, f)) and \\\n",
      "             f[0] != '.' ] #ignore hidden files\n",
      "    return sorted(files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 248
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Get all subdirectories in a directory\n",
      "def get_all_dirs(path):\n",
      "    folders = [f for f in listdir(path) if isdir(join(path, f)) and \\\n",
      "               f[0] != '.'] #ignore hiddin directories\n",
      "    return sorted(folders)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 249
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get all signal files for movie in directory\n",
      "def get_images(movie, signal, data_path):\n",
      "    path = data_path + movie + '/' + signal + '/'\n",
      "    files = get_all_files(path)\n",
      "    image_dict = {}\n",
      "    \n",
      "    for f in files:\n",
      "        im = Image.open(path + f)\n",
      "        im.load()\n",
      "        image_dict[int(f[:-4])] = convert(signal, im)\n",
      "        \n",
      "    return image_dict\n",
      "\n",
      "#get_images('anni005', 'sample_signal')\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 250
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_movie_signals(movie, data_path):\n",
      "    mov_sigs = {}\n",
      "    for s in SIGNALS:\n",
      "        mov_sigs[s] = get_images(movie, s, data_path)\n",
      "        \n",
      "    #now vstack them. \n",
      "    mov_compiled = mov_sigs[SIGNALS[0]]\n",
      "    \n",
      "    for f in mov_compiled:\n",
      "        for s in SIGNALS[1:]:\n",
      "            mov_compiled[f] =  np.vstack(mov_compiled[f], mov_sigs[s])\n",
      "            \n",
      "    return mov_compiled\n",
      "    \n",
      "    \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 251
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_all_sigs(data_path):\n",
      "    dirs = get_all_dirs(data_path)\n",
      "    print('Found ' + str(len(dirs)) + ' movies: ')\n",
      "    print(dirs)\n",
      "    all_sigs = {}\n",
      "    for d in dirs:\n",
      "        all_sigs[d] = get_movie_signals(d, data_path)\n",
      "    \n",
      "    return all_sigs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 252
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Read shot_annot csv files\n",
      "\n",
      "# Dictionary of all movies\n",
      "# Each movie will have a list of ShotAnnotations\n",
      "# read the files into dictionary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def get_labels(label_path):\n",
      "    \n",
      "    files = [f for f in listdir(label_path) if isfile(join(label_path, f)) and f[0] != '.']\n",
      "    annotations = {}\n",
      "    annot_arrays = {}\n",
      "    colNames = ['style', 'pre_frame', 'post_frame']\n",
      "    \n",
      "    for f in files:\n",
      "        full_path = label_path + f\n",
      "        name = (basename(f)).split('.')[0] #get only movie title\n",
      "        annotations[name] = []\n",
      "        annot_arrays[name] = {}\n",
      "        with open(label_path + f, 'rt') as csvfile:\n",
      "            reader = csv.reader(csvfile, delimiter=',')\n",
      "            next(reader)    \n",
      "            idx = 0\n",
      "            for row in reader:\n",
      "                #new_annot = ShotAnnotation(f, row[0], row[1], row[2])\n",
      "                annotations[name].append(new_annot)\n",
      "                if(row[0] == 'CUT'):\n",
      "                    annot_arrays[name][row[1]] = 1\n",
      "\n",
      "    #Debug check: did the dictionary populate as expected?        \n",
      "    print('Gathered labels for ',len(annot_arrays), 'videos')\n",
      "    return annot_arrays\n",
      "    \n",
      "    \n",
      "#get_labels()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Make the final image and label arrays\n",
      "\n",
      "def prep_data(data_path, label_path):\n",
      "    signals = get_all_sigs(data_path)\n",
      "    labels = get_labels(label_path)\n",
      "    \n",
      "    final_labels = []\n",
      "    signal_ndArrays = np.empty((1, SIG_COUNT, FORCED_HEIGHT, FORCED_WIDTH))\n",
      "    for m in sorted(signals):\n",
      "        max_val = max([int(k) for k in signals[m]])\n",
      "        \n",
      "        these_labels = [0] * (max_val - 1)\n",
      "        for cut in labels[m]:\n",
      "            #-1 because frames are indexed starting at 1\n",
      "            #these_labels[int(cut) - 1] = labels[cut]\n",
      "            x = 1\n",
      "        final_labels.append(these_labels)\n",
      "        itermov = iter(sorted(signals[m]))\n",
      "        \n",
      "        if(len(signal_ndArrays) == 1):\n",
      "            signal_ndArrays[0,:,:,:] = signals[m][1]\n",
      "            next(itermov)\n",
      "            \n",
      "        for sig in itermov:\n",
      "            new_array = np.empty((1, SIG_COUNT, FORCED_HEIGHT, FORCED_WIDTH))\n",
      "            new_array[0,:,:,:] = signals[m][sig]\n",
      "            signal_ndArrays = np.vstack((signal_ndArrays, new_array))\n",
      "            \n",
      "    print final_labels\n",
      "    print(signal_ndArrays.shape)\n",
      "    \n",
      "    return{'labels':final_labels, 'signals':signal_ndArrays}\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 254
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 254
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create the FrameDataSet Class\n",
      "class FrameDataSet(dense_design_matrix.DenseDesignMatrix):\n",
      "    \n",
      "    def __init__(self, data_path, label_path):\n",
      "        data = prep_data(data_path, label_path)\n",
      "        labels = data['labels']\n",
      "        ims = data['signals']\n",
      "        \n",
      "        self.num_ims, self.ch, self.h, self.w = ims.shape\n",
      "        self.img_shape = (self.ch, self.h, self.w)\n",
      "        start_idx = 0\n",
      "        self.max_count = sys.maxsize\n",
      "        self.label_names = [0, 1]\n",
      "        self.n_classes = len(self.label_names)    \n",
      "            \n",
      "            \n",
      "        #The following parts might not be necessary.\n",
      "        self.one_hot = False\n",
      "        self.gcn = 55. #Don't know what this does.\n",
      "        \n",
      "    \n",
      "        def get_labels():\n",
      "            return labels\n",
      "        \n",
      "        self.label_map = {k: v for k, v in zip(self.label_names,\\\n",
      "                                               range(self.n_classes))}\n",
      "        self.label_unmap = {v: k for k, v in zip(self.label_names,\\\n",
      "                                               range(self.n_classes))}\n",
      "        \n",
      "        \n",
      "        \n",
      "                                         \n",
      "            \n",
      "    \n",
      "            \n",
      "        \n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 255
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#all_data = FrameDataSet(TRAIN_DATA, TRAIN_LABELS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 260
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "from pylearn2.models import mlp, maxout\n",
      "from pylearn2.costs.mlp.dropout import Dropout\n",
      "from pylearn2.training_algorithms import sgd, learning_rule\n",
      "from pylearn2.termination_criteria import EpochCounter\n",
      "from pylearn2.datasets.preprocessing import Pipeline, ZCA\n",
      "from pylearn2.datasets.preprocessing import GlobalContrastNormalization\n",
      "from pylearn2.space import Conv2DSpace\n",
      "from pylearn2.train import Train\n",
      "from pylearn2.train_extensions import best_params, window_flip\n",
      "from pylearn2.utils import serial\n",
      "\n",
      "#Define Datasets here\n",
      "trn = FrameDataSet(TRAIN_DATA, TRAIN_LABELS)\n",
      "\n",
      "tst = FrameDataSet(TEST_DATA, TEST_LABELS)\n",
      "\n",
      "\n",
      "#Define the network here\n",
      "in_space = Conv2DSpace(shape=(32, 32),\n",
      "                       num_channels=3,\n",
      "                       axes=('c', 0, 1, 'b'))\n",
      "\n",
      "l1 = maxout.MaxoutConvC01B(layer_name='l1',\n",
      "                           pad=4,\n",
      "                           tied_b=1,\n",
      "                           W_lr_scale=.05,\n",
      "                           b_lr_scale=.05,\n",
      "                           num_channels=96,\n",
      "                           num_pieces=2,\n",
      "                           kernel_shape=(8, 8),\n",
      "                           pool_shape=(4, 4),\n",
      "                           pool_stride=(2, 2),\n",
      "                           irange=.005,\n",
      "                           max_kernel_norm=.9,\n",
      "                           partial_sum=33)\n",
      "\n",
      "l2 = maxout.MaxoutConvC01B(layer_name='l2',\n",
      "                           pad=3,\n",
      "                           tied_b=1,\n",
      "                           W_lr_scale=.05,\n",
      "                           b_lr_scale=.05,\n",
      "                           num_channels=192,\n",
      "                           num_pieces=2,\n",
      "                           kernel_shape=(8, 8),\n",
      "                           pool_shape=(4, 4),\n",
      "                           pool_stride=(2, 2),\n",
      "                           irange=.005,\n",
      "                           max_kernel_norm=1.9365,\n",
      "                           partial_sum=15)\n",
      "\n",
      "l3 = maxout.MaxoutConvC01B(layer_name='l3',\n",
      "                           pad=3,\n",
      "                           tied_b=1,\n",
      "                           W_lr_scale=.05,\n",
      "                           b_lr_scale=.05,\n",
      "                           num_channels=192,\n",
      "                           num_pieces=2,\n",
      "                           kernel_shape=(5, 5),\n",
      "                           pool_shape=(2, 2),\n",
      "                           pool_stride=(2, 2),\n",
      "                           irange=.005,\n",
      "                           max_kernel_norm=1.9365)\n",
      "\n",
      "l4 = maxout.Maxout(layer_name='l4',\n",
      "                   irange=.005,\n",
      "                   num_units=500,\n",
      "                   num_pieces=5,\n",
      "                   max_col_norm=1.9)\n",
      "\n",
      "output = mlp.Softmax(layer_name='y',\n",
      "                     n_classes=10,\n",
      "                     irange=.005,\n",
      "                     max_col_norm=1.9365)\n",
      "\n",
      "layers = [l1, l2, l3, l4, output]\n",
      "\n",
      "#Define the model\n",
      "#Not sure if we need to modify this.\n",
      "mdl = mlp.MLP(layers,\n",
      "              input_space=in_space)\n",
      "\n",
      "\n",
      "#Not sure what this does from here to next comment.\n",
      "trainer = sgd.SGD(learning_rate=.17,\n",
      "                  batch_size=128,\n",
      "                  learning_rule=learning_rule.Momentum(.5),\n",
      "                  # Remember, default dropout is .5\n",
      "                  cost=Dropout(input_include_probs={'l1': .8},\n",
      "                               input_scales={'l1': 1.}),\n",
      "                  termination_criterion=EpochCounter(max_epochs=475),\n",
      "                  monitoring_dataset={'valid': tst,\n",
      "                                      'train': trn})\n",
      "\n",
      "preprocessor = Pipeline([GlobalContrastNormalization(scale=55.), ZCA()])\n",
      "trn.apply_preprocessor(preprocessor=preprocessor, can_fit=True)\n",
      "tst.apply_preprocessor(preprocessor=preprocessor, can_fit=False)\n",
      "serial.save('deepcuts_preprocessor.pkl', preprocessor)\n",
      "\n",
      "watcher = best_params.MonitorBasedSaveBest(\n",
      "    channel_name='valid_y_misclass',\n",
      "    save_path='deepcuts_maxout_zca.pkl')\n",
      "\n",
      "velocity = learning_rule.MomentumAdjustor(final_momentum=.65,\n",
      "                                          start=1,\n",
      "                                          saturate=250)\n",
      "\n",
      "decay = sgd.LinearDecayOverEpoch(start=1,\n",
      "                                 saturate=500,\n",
      "                                 decay_factor=.01)\n",
      "\n",
      "win = window_flip.WindowAndFlipC01B(pad_randomized=8,\n",
      "                                    window_shape=(32, 32),\n",
      "                                    randomize=[trn],\n",
      "                                    center=[tst])\n",
      "\n",
      "#Define experiment\n",
      "experiment = Train(dataset=trn,\n",
      "                   model=mdl,\n",
      "                   algorithm=trainer,\n",
      "                   extensions=[watcher, velocity, decay, win])\n",
      "\n",
      "#Run experiment\n",
      "experiment.main_loop()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 1 movies: \n",
        "['anni005']\n",
        "('Gathered labels for ', 13, 'videos')\n",
        "[[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
        "(4, 3, 512, 512)\n",
        "Found 1 movies: \n",
        "['anni005']\n",
        "('Gathered labels for ', 13, 'videos')\n",
        "[[0, 0, 0, 0, 0, 0, 0, 0, 0]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(4, 3, 512, 512)\n"
       ]
      },
      {
       "ename": "RuntimeError",
       "evalue": "<class 'pylearn2.models.maxout.MaxoutConvC01B'> only runs on GPUs, but there doesn't seem to be a GPU available. If you would like assistance making a CPU version of convolutional maxout, contact pylearn-dev@googlegroups.com.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-259-ed6445318f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                            \u001b[0mirange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.005\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                            \u001b[0mmax_kernel_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                            partial_sum=33)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m l2 = maxout.MaxoutConvC01B(layer_name='l2',\n",
        "\u001b[0;32m/home/alex/pylearn2/pylearn2/models/maxout.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_channels, num_pieces, kernel_shape, pool_shape, pool_stride, layer_name, irange, init_bias, W_lr_scale, b_lr_scale, pad, fix_pool_shape, fix_pool_stride, fix_kernel_shape, partial_sum, tied_b, max_kernel_norm, input_normalization, detector_normalization, min_zero, output_normalization, kernel_stride)\u001b[0m\n\u001b[1;32m    637\u001b[0m                  \u001b[0moutput_normalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                  kernel_stride=(1, 1)):\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mcheck_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxoutConvC01B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/alex/pylearn2/pylearn2/sandbox/cuda_convnet/__init__.py\u001b[0m in \u001b[0;36mcheck_cuda\u001b[0;34m(feature_name, check_enabled)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;34m\"seem to be a GPU available. If you would like assistance making \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;34m\"a CPU version of convolutional maxout, contact \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \"pylearn-dev@googlegroups.com.\" % feature_name)\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_ndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cublas_v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mRuntimeError\u001b[0m: <class 'pylearn2.models.maxout.MaxoutConvC01B'> only runs on GPUs, but there doesn't seem to be a GPU available. If you would like assistance making a CPU version of convolutional maxout, contact pylearn-dev@googlegroups.com."
       ]
      }
     ],
     "prompt_number": 259
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}